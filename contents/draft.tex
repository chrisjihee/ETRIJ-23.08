\documentclass[AMS,STIX2COL]{WileyNJD-v2}

\articletype{Article Type}
\received{26 April 2016}
\revised{6 June 2016}
\accepted{6 June 2016}
\raggedbottom

\begin{document}
    \title{Improvement of Korean Morphological Analysis System \\Through Transformer-Based Re-Ranking}

    \author[1]{Author One*}
    \author[2,3]{Author Two}
    \author[3]{Author Three}
    \authormark{AUTHOR ONE \textsc{et al}}

    \address[1]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}
    \address[2]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}
    \address[3]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}
    \corres{*Corresponding author name, This is sample corresponding address. \email{authorone@gmail.com}}
    \presentaddress{This is sample for present address text this is sample for present address text}

    \abstract[Abstract]{
        This study presents a novel approach to Korean morphological analysis by integrating a Transformer-based re-ranking method into a dictionary-based morphological analysis system.
        A significant advancement in this research is the effective utilization of the BERT model's re-ranking, which has brought innovations in the broader field of natural language processing.
        This enhancement is achieved through a two-stage re-ranking process: the first stage involves fine-tuning a BERT model using only the results of morphological analysis, and the second stage fine-tunes a different type of BERT model using the re-ranked results along with the original text.
        This method effectively improves the performance of dictionary-based morphological analysis, which previously lagged behind syllable-based analysis in terms of accuracy.
        The integration of deep learning with traditional morphological analysis represents a significant advancement in providing more powerful and efficient tools for language processing tasks.
        According to our results, there is a substantial improvement in eojeol accuracy and morpheme F1 scores compared to previous methods.
        This study not only sets a new standard in Korean language processing but also paves the way for further advancements in applying deep learning to linguistic analysis.
    }
    \keywords{Korean morphological analysis, natural language understanding, deep learning, pretrained transformer encoder, re-ranking}

    \JELinfo{classification}
    \MSC{Code numbers}
    \jnlcitation{\cname{
        \author{Williams K.},
        \author{B. Hoskins},
        \author{R. Lee},
        \author{G. Masato}, and \author{T. Woollings}} (\cyear{2016}),
        \ctitle{A regime analysis of Atlantic winter jet variability applied to evaluate HadGEM3-GC2}, \cjournal{Q.J.R. Meteorol. Soc.}, \cvol{2017;00:1--6}.}
    \maketitle
    \footnotetext{\textbf{Abbreviations:} ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor}


    \begin{table*}[t]
        \caption{Maximum performance of alternative paths as correct answers}
        \label{tab:maximum-performance}
        \centering
        \begin{tabular*}{500pt}{@{\extracolsep\fill}cccD{.}{.}{3}c@{\extracolsep\fill}}
            \toprule
            ~                 & \multicolumn{2}{@{}c@{}}{Written Language Evaluation Set} & \multicolumn{2}{@{}c@{}}{Spoken Language Evaluation Set}                  \\ \cmidrule{2-3} \cmidrule{4-5}
            alternative range & eojeol accuracy & average number of alternative           & \multicolumn{1}{@{}l@{}}{eojeol accuracy} & average number of alternative \\
            \midrule
            no alternative    & 96.36           & 1.0                                     & 92.54                                     & 1.0                           \\
            secondary         & 98.74           & 25.7                                    & 97.27                                     & 12.9                          \\
            tertiary          & 98.96           & 47.8                                    & 97.81                                     & 23.6                          \\
            quarternary       & 99.01           & 69.6                                    & 97.95                                     & 34.2                          \\
            quinary           & 99.02           & 91.1                                    & 98.01                                     & 44.5                          \\
            \bottomrule
        \end{tabular*}
        \begin{tablenotes}
            \footnotesize
            \item\hspace{2mm} * Written Language Evaluation Set: 2,400 sentences each randomized from UCorpus and Everyone's Corpus (4,800 sentences total)
            \item\hspace{2mm} * Spoken Language Evaluation Set: 2,400 sentences each randomized from UCorpus and Everyone's Corpus (4,800 sentences total)
        \end{tablenotes}
    \end{table*}

    \section{Introduction}\label{sec:intro}

    Korean morphological analysis involves determining parts of speech by identifying morphemes, the smallest units of linguistic expression with independent meanings in a sentence.
    Unlike isolating languages like English, where sequential tagging suffices, Korean, being agglutinative, requires separating endings or postpositions and restoring inflections.
    The accuracy of morphological analysis significantly impacts Korean analysis performance since many tasks rely on separate morphemes as their basic input.
    Modern deep learning methods in natural language processing use tokenization, breaking text into smaller units and converting each into a vector for computational models~\cite{Mikolov2013}.
    For Korean, where subword units are crucial, attempting tokenization with separate morphemes in advance reflects the language's characteristics~\cite{SongHJ2021}.
    Incorporating morphological analysis results into this process enhances overall performance, capturing the semantic units of Korean.
    To accomplish this, we need a morphological analyzer that is not only highly accurate but also operates swiftly.

    Several approaches have been suggested for morphological analysis, a critical aspect of Korean language comprehension~\cite{KwonHC1991, LeeDG2009, ShimKS2011, LeeJS2011, ShinJC2012, LeeCK2013, NaSH2014, NaSH2015, HwangHS2016, KimHM2016, ChungES2016, LeeCH2016, Li2017, NaSH2018, KimSW2018, ChoiYS2018, MinJW2018, MinJW2019, KimHM2019, SongHJ2019, MinJW2020, SongHJ2020, ChoiYS2020, HwangHS2020, KimHJ2021, YounJY2021, MinJW2022, KimJM2022, ShinHJ2023}.
    Typically, when individuals grasp spoken or written language, they try to comprehend it through familiar vocabulary and concepts.
    While some approaches rely on rules or dictionaries to capture this understanding~\cite{KwonHC1991}, constructing and updating dictionaries for varied text vocabularies can be challenging.
    As a result, methods focusing on tagging syllable units without a dictionary have been proposed~\cite{ShimKS2011, LeeCK2013, LeeCH2016, KimHM2016} and studied for enhancement~\cite{KimSW2018, ChoiYS2018, KimHM2019, MinJW2019, SongHJ2019, SongHJ2020, YounJY2021, ShinHJ2023}.
    Mechanically, syllable-by-syllable morphological analysis can be achieved by either tagging syllables and then applying a base-form restoration dictionary~\cite{ShimKS2011, LeeCH2016} or by tagging syllables with the base form pre-restored~\cite{YounJY2021}.
    However, this approach has limitations, struggling with precise morpheme boundary identification and struggling to grasp long-term contextual information as the sequence lengthens.
    In this study, the former is termed dictionary-based morphological analysis, and the latter is syllable-unit morphological analysis.
    Both methods are trained on manually labeled corpora, facing challenges in accurately analyzing new syllable combinations or morphemes absent in the training data.
    The evolution of the Internet, open sources, and shared knowledge has led to substantial accumulations of web texts, corpora, language resources, offering an opportunity to overcome the constraints of dictionary-based methods due to reduced costs in dictionary construction and maintenance.

    Given this context, our study aims to enhance the effectiveness of the dictionary-based morphological analysis method employed by MeCab~\cite{MeCab}, an open-source tool for Korean and Japanese morphological analysis commonly used as a crucial preprocessing tool for deep learning.
    The method, trained through Conditional Random Fields (CRF)~\cite{Lafferty2001}, generates a lattice structure from a given sentence, connecting candidate morphemes in the dictionary through a directed graph.
    Subsequently, the optimal morphological analysis path is determined within this lattice structure~\cite{Kudo2004, NaSH2014, NaSH2018}.
    The Viterbi algorithm~\cite{Viterbi1967} is employed in this process, minimizing the cost associated with each morpheme node and the sum of neighborhood costs for consecutive morphemes to identify the optimal path.

    In these dictionary-based morphological analysis methods, the primary errors stem from encountering new words absent in the dictionary within a sentence or when biases lead to the selection of an incorrect result during optimal path calculation.
    For instance, opting for one long morpheme over several short ones might be cost-effective but often results in an inaccurate analysis.
    The main impetus behind our study is the recognition that the path minimizing costs for nodes and links may not always align with the optimal path.
    In response, we propose methods to address these challenges and improve the accuracy of the morphological analysis process.

    \begin{table*}[h]
        \centering
        \footnotesize
        \caption{Statistics for the Korean morphological corpus as a whole and for training/test data}
        \label{tab:data-statistics}
        \begin{tabular}{m{20mm}m{14mm}m{10mm}m{12mm}m{10mm}m{10mm}m{12mm}m{10mm}m{10mm}m{12mm}m{10mm}}
            \toprule
            \multirow{2}{*}{Corpus}            & \multirow{2}{*}{Style} & \multicolumn{3}{c}{Raw Data}                 & \multicolumn{3}{c}{Training Data}            & \multicolumn{3}{c}{Test Data}             \\
            \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
            ~                                  & ~                      & sentences & eojeols    & morphs\newline/sent & sentences & eojeols    & morphs\newline/sent & sentences & eojeols & morphs\newline/sent \\
            \midrule
            Sejong Corpus                      & written                & 854,475   & 10,052,869 & 26.8                & 194,822   & 2,681,582  & 31.0                & 49,922    & 678,578 & 30.6                \\
            \midrule
            \multirow{3}{*}{UCorpus}           & written                & 5,456,101 & 62,462,158 & 25.1                & 4,998,560 & 57,393,332 & 25.4                & 53,003    & 598,413 & 25.0                \\
            ~                                  & semi-spoken            & 393,770   & 3,401,444  & 18.4                & 334,061   & 2,960,146  & 19.4                & 38,960    & 332,285 & 18.6                \\
            ~                                  & spoken                 & 627,380   & 2,819,427  & 10.9                & 429,215   & 2,295,940  & 13.0                & 62,399    & 279,545 & 11.1                \\
            \midrule
            \multirow{2}{*}{Everyone's Corpus} & written                & 150,082   & 2,000,213  & 30.4                & 129,352   & 1,713,367  & 30.5                & 14,442    & 191,223 & 30.5                \\
            ~                                  & spoken                 & 221,371   & 1,006,287  & 8.7                 & 137,869   & 714,021    & 10.5                & 19,789    & 85,316  & 8.6                 \\
            \bottomrule
        \end{tabular}
    \end{table*}

    To pinpoint instances where a suboptimal solution may, in fact, be the best choice according to the best path calculation, we modified the best path calculation method to yield suboptimal analysis results and assessed their accuracy.
    While various approaches exist for selecting the next-best path, we opted for the method of substituting a morpheme node on the optimal path with a lower-ranked node.
    Table~\ref{tab:maximum-performance} illustrates the degree to which analysis performance can be enhanced by replacing the optimal path with a lower-ranked node.
    This problem is analogous to the challenge of re-ranking search results in information retrieval~\cite{BaeYJ2021}, where the goal is to identify the correct answer among the generated suboptimal results.

    In ~\cite{ChoiYS2018}, the N-best analysis results produced by the seq2seq model were re-ranked based on a convolutional neural network to enhance performance.
    In our study, we employed re-ranking with two distinct BERT models, each of different types and forms, as proposed in ~\cite{Nogueira2019}.
    Experimental results reveal that first-stage re-ranking improves performance by over 20\% compared to existing written and spoken models.
    Furthermore, second-stage re-ranking, incorporating a different input type and a diverse pre-trained model, contributes to a performance improvement exceeding 30\% compared to existing written and spoken models.

    While our introduced method led to further enhancement in the performance of the dictionary-based morphological analysis, it resulted in an overall increase in analysis time when configuring the morphological analysis system, including the re-ranking model itself.
    However, a promising avenue for future exploration lies in utilizing the results of multiple re-ranked morpheme analyses to update the connection costs between morphemes in a dictionary, akin to the backpropagation process in a typical neural network.
    It is anticipated that an improved morphological analysis system with updated connection costs can generate superior re-ranking candidates, potentially enabling iterative performance improvements.
    While this study focused on two-stage re-ranking, further research is essential to fully explore this potential.

    The primary contributions of this study can be summarized as follows:
    \begin{enumerate}
        \item \textbf{Further improvement of dictionary-based morphological analysis method using suboptimal analysis results}: We investigate the potential for performance improvement by introducing a method to replace the optimal path with a suboptimal node. Additionally, we propose an effective approach to enhance the dictionary-based morphological analysis method through deep learning.
        \item \textbf{Extending the performance improvement by introducing a two-stage re-ranking model}: To further enhance the performance of dictionary-based analysis through re-ranking, we suggest extending the improvement using different BERT models and conducting two rounds of re-ranking.
        \item \textbf{A method for updating connection costs in the dictionary and suggestions for future research}: We present a novel method for updating dictionary connection costs based on re-ranked morphological analysis results. Furthermore, we outline directions for future research, suggesting potential enhancements.
    \end{enumerate}
    These contributions provide valuable insights into advancing the performance of Korean morphological analysis and offer guidance for future researchers.

    The subsequent sections of this paper are organized as follows:
    Section ~\ref{sec:morphological-analysis-model} discusses the configuration and training of a dictionary-based morphological analysis system.
    Section ~\ref{sec:reranking-model} covers the generation of secondary results of morphological analysis, the production of re-ranking data, and the proposal of a method for training a two-stage re-ranking model.
    Section ~\ref{sec:results} delves into the results of the performance improvement using morphological analysis and re-ranking models.
    Section ~\ref{sec:related-work} introduces previous research cases related to this study.
    Finally, in Section ~\ref{sec:conclusion}, we conclude the study, discuss its limitations, and suggest directions for future research.


    \begin{figure*}[t]
        \centerline{\includegraphics[width=0.75\textwidth]{fig1.1}}
        \caption{Transformation of a single sentence in the Korean morpheme-tagged corpus into a single training sample}
        \label{fig:sample}
    \end{figure*}


    \begin{figure*}[t]
        \centerline{\includegraphics[width=0.9\textwidth]{fig2.1}}
        \caption{Example of lattice construction and decoding result and secondary path generation}
        \label{fig:lattice}
    \end{figure*}


    \section{Morphological Analysis Model}\label{sec:morphological-analysis-model}

    \subsection{Korean Morphological Analysis Corpora}\label{subsec:korean-morphological-analysis-corpora}

    In this study, we used three major corpora to train and evaluate Korean morphological analysis models, each of which has unique characteristics and serves different research purposes.

    \textbf{Sejong Corpus}: Originating from the 21st Century Sejong Project, this corpus consists of a total of 15 million eojeols, including the raw untagged corpus, and forms the backbone of Korean morphological analysis research~\cite{ChoeMW2008}.
    It provides a wide variety of linguistic patterns and structures that are important for baseline training and validation of morphological analysis models, and has been used for performance comparisons with other studies.
    In most of the previous studies, only a part of the Sejong corpus was used for experiments, and in this study, we used the dataset provided by the researchers in ~\cite{NaSH2014}.

    \textbf{UCorpus (University of Ulsan Corpus)}~\cite{UCorpusHG}: This is an extension of the Sejong corpus, which is constantly being maintained and added to by the University of Ulsan, significantly increasing its volume to 63 million eojeols and testing the adaptability and accuracy of the model to a wider range of data.
    The expansion includes corrections to previously raised errors ~\cite{KimIH2010} and a number of annotation outputs for new data, providing a substantial basis for comprehensive linguistic analysis.

    \textbf{Everyone's Corpus}~\cite{EveryoneCorpus}: Launched by the National Institute of the Korean Language in 2020, the Everybody's Corpus enriches the data landscape with web texts and spoken language materials that reflect contemporary language use~\cite{KimIH2019}.
    This modern corpus reflects the dynamic evolution of the Korean language and is playing a pivotal role in improving models for capturing the nuances of current Korean usage.

    Table~\ref{tab:data-statistics} details the specific number of sentences and words in each corpus, and the data subsets extracted for model training and evaluation.
    During the training data conversion process, we first removed duplicate sentences and excluded sentences with annotation errors or other problems.
    We can see that many duplicate sentences occur, especially in spoken language.

    \subsection{Training Example Transformation}\label{subsec:training-example-transformation}

    To train a dictionary-based morpheme analysis model effectively, the morpheme-tagged corpus, typically represented in lemma form, must be transformed to include the boundary information between morphemes in its surface form.
    Crucial to this transformation is string alignment, a process that accounts for the discrepancies between lemma forms and surface forms in the Korean morphological analysis corpus.

    In this study, string alignment was performed using the Smith-Waterman algorithm, which uses a scoring matrix based on the similarity of the grapheme unit of Korean letters for each word pair (as shown in Figure~\ref{fig:sample}). Each aligned sentence containing a morpheme tag was converted into a training sample tailored for dictionary-based morphological analysis.

    The resulting table in Figure~\ref{fig:sample} illustrates this process.
    Each row acts as a lexical unit.
    The first four columns contribute to feature generation, whereas the last four columns facilitate post-lemmatization.
    Using the morphological corpus above, a large number of training samples can be generated according to the process shown in Figure~\ref{fig:sample}.
    With the exception of the evaluation samples, the remaining sentences were used to train the dictionary-based morphological analysis model using the CRF algorithm.
    The output of this training allowed the calculation of the costs associated with each morpheme node and the linking of two consecutive morphemes.
    This, in turn, allowed the discovery of an optimal path using the Viterbi algorithm.

    \begin{figure*}[t]
        \centerline{\includegraphics[width=0.9\textwidth]{fig3.0}}
        \caption{Two-stage re-ranking model for Korean morphological analysis}
        \label{fig:ranking}
    \end{figure*}

    \subsection{Lattice Construction and Decoding}\label{subsec:lattice-construction-and-decoding}

    Figure~\ref{fig:lattice} presents a snapshot of the lattice structure, which is an integral part of the morphological analysis.
    (1) shows a fragment of the lattice structure formed when the example sentence in Figure~\ref{fig:sample} is entered.
    (2) shows the optimal path determined using the Viterbi algorithm.

    However, the path inferred by the trained model may differ from the correct solution constructed by humans.
    The nodes marked with stars in (1) represent correct nodes.
    The upper-left number of each node indicates the ranking of the nodes accessible at each decoding point.
    The choices made at certain moments deviate from the correct solution.
    To improve analytical performance, mechanisms to correct these discrepancies must developed.


    \section{Re-ranking Model}\label{sec:reranking-model}

    Although dictionary-based morphological analysis offers significant advances, its optimal paths occasionally diverge from the correct solutions that humans understand.
    This divergence underscores the need for a model that reevaluates these primary results and reorients them to achieve higher accuracy.
    This method, called re-ranking, involves generating multiple analyses of an input and then reordering them based on a new set of criteria or models, thereby improving the overall quality of the results.

    \subsection{Secondary Path Generation}\label{subsec:secondary-path-generation}

    Before reranking begins, multiple analyses, typically referred to as the N-best paths, of the input sentence are generated.
    This involves extracting the top N atoms from the lattice structure.
    In this study, a novel approach is introduced to generate secondary paths, as shown in (3) of Figure~\ref{fig:lattice}, by selecting the second-best node rather than each best node constituting the path from the best-path result.
    Some of these secondary paths provided alternatives that reconciled incorrect with correct answers.
    Similarly, paths modified by favoring the third-best node were called tertiary paths, and this naming convention was continued for subsequent paths.
    In our preliminary test, the secondary paths, including the optimal and suboptimal paths, were shown to cover the majority of the correct morphological analyses as measured through human evaluations.
    (Refer to Table~\ref{tab:maximum-performance}).

    \subsection{BERT-based Re-ranking}\label{subsec:bert-based-reranking}

    Bidirectional Encoder Representations from Transformers (BERT) models~\cite{Devlin2019} have revolutionized many natural language processing tasks by understanding the context in which words appear in text.
    In this study, we attempt to leverage the power of BERT to reorder the generated secondary paths.
    We labeled the generated secondary paths with scores related to the morphological analysis performance to fine-tune a pre-trained BERT model specialized for Koreans with excess amount Korean text.
    After pre-testing several scoring methods on a modest scale, we found that using scores based on the degree of error rather than scores based on accuracy by widen the gap between correct and incorrect answers is effective for learning.

    Once the BERT model is fine-tuned and trained for the re-ranking task, it can predict a re-ranking score for each path in the secondary path list.
    This implies that considering the context, morphological organization, and other essential linguistic features of the path, the model assigns a score to each path.
    The paths were then re-ranked according to this score, and the path with the highest score was selected for the best morphological analysis.

    \subsection{Two-stage Re-ranking}\label{subsec:two-stage-reranking}

    Given the complexity of the Korean language, a single re-ranking step does not constantly yield accurate results.
    Therefore, we propose a two-step re-ranking approach as described in ~\cite{Nogueira2019}.

    In the first step, we re-rank the secondary paths generated using the BERT model, as described in Section ~\ref{subsec:bert-based-reranking}.
    In the second step, we introduced another BERT variant that was optimized for a different set of linguistic features or trained on a different dataset.
    This allowed us to perform a fine-grained re-evaluation, further refine the list, and push more contextually accurate paths to the top.

    As shown in Figure~\ref{fig:ranking}, for a two-stage reranking model, the first stage performs the first re-ranking, taking as input a secondary path in morphologically tagged lemma form.
    It then performs a second re-ranking, again taking as input the path re-ranked in stage 1 and the original input sentence.
    This was conducted to improve effectiveness, as it was ineffective given the same type of input.

    In summary, the re-ranking model represents a significant advancement in our approach to Korean morphological analysis.
    By leveraging BERT-based models, we anticipate a marked improvement in accuracy, especially in complex linguistic scenarios.
    The next section will present our experimental setup and results, providing crucial empirical evidence for the efficacy of the re-ranking model in practical applications.


    \section{Experimental Results}\label{sec:results}

    Having developed the re-ranking model as a theoretical framework for enhancing Korean morphological analysis, we now turn to empirical validation.
    This section outlines our experimental setup, designed meticulously to test the performance of our model.
    Through these experiments, we aim to demonstrate not just the model's accuracy but also its practical applicability in handling the nuances of Korean language processing.

    We evaluated the performance of the proposed deep learning-integrated dictionary-based morphological analysis method.
    This section presents the results of the experimental evaluation considering the improvements over conventional methods and the effectiveness of our re-ranking model.

    \subsection{Setup and Data}\label{subsec:setup-and-data}

    For our experiments, we used the Sejong corpus (versions used in ~\cite{NaSH2014, NaSH2015, NaSH2018, SongHJ2019, SongHJ2020}), UCorpus\cite{UCorpusHG}, and Everyone's Corpus\cite{EveryoneCorpus}.
    For comparison with previous studies, the Sejong corpus was trained using a single model without separation.
    The UCorpus and Everyone's Corpus provided a separate spoken corpus with drama scripts and broadcast dialogues, while UCorpus separated documents that were considered to be close to spoken language and further organized them into a semi-spoken corpus.
    Because UCorpus and Everyone's Corpus have synergistic effects when trained concurrently, we trained the models separately for written and spoken language rather than separating them by source.
    The statistics of the full data for the three types of models are presented in Table~\ref{tab:data-statistics}.
    Because of the large volume of UCorpus, we randomly selected some of them to train the actual model.

    We transformed this organized morphological corpus using the training-example transformation process described in Section ~\ref{subsec:training-example-transformation} to generate samples for training the dictionary-based morphological analysis model.

    \subsection{Evaluation Metrics}\label{subsec:evaluation-metrics}

    To measure the accuracy of the morphological analysis model, correctness of the N-best path, and ranking accuracy of the reranking model, we used the eojeol accuracy and morpheme F1 scores as evaluation metrics.
    To verify that they produced the correct morphological analysis results, we measured the degree of agreement with human annotations on the corpus.
    However, owing to the slightly different criteria and styles of the annotators who labeled the different types of corpora, including the comparison with the MeCab-ko system, the following adjustments were made:
    \begin{itemize}
        \item Sentences containing unanalyzable tags (NF, NA, and NV) were excluded from both training and evaluation.
        \item As for the tagsets, we excluded three unanalyzable tags from the 45 Sejong tagsets and used 42 tagsets.
        \item Each tag output by the MeCab-ko system was converted to the corresponding tag in the Sejong tagset.
        \item Chinese characters were converted to Chinese character tags (SH) even if they were semantically used as nouns, and consecutive Chinese characters were converted to a single morpheme.
        \item Similarly, symbol, numeral, ending, and postposition in the same tag were converted to a single morpheme, and decimal expressions were treated as a single morpheme, including the midpoint and the numbers before and after.
        \item If the first lemma letter of the ending is `[eo]', `[yeo]', or `[ah]', it is unified as `[eo]', and if it is `[eot]', `[yeot]', or `[ass]', it is unified as `[eot]'. %`어[eo]', `여[yeo]', or `아[ah]' % `었[eot]', `였[yeot]', or `았[ass]'
        \item Root tags (XR) used alone without affixes were replaced with common nouns (NNG) because they are mainly used in the Sejong corpus only.
        \item As mentioned in ~\cite{KimIH2010}, connective endings (EC) and sentence-closing endings (EF) are not clearly defined in the tagging guidelines, and there are cases where they are used interchangeably in the corpus, to ensure that we evaluated them without distinguishing them.
        \item The distinction between `[geot]' and `[geo]' is unclear in the tagging guidelines, and there are cases where they are used interchangeably in the corpus, hence, we did not distinguish between them. %`것[geot]' and `거[geo]'
        \item Compound words can be interpreted as a single morpheme or as a combination of two or more morphemes or affixes, hence, we evaluated them without distinguishing between them.
        \item Proper nouns can also be interpreted as common nouns depending on the point of view or perspective. Human annotators have slightly different standards, and thus they were also evaluated without distinguishing the nouns.
    \end{itemize}

    \subsection{Basic Performance}\label{subsec:basic-performance}

    First, we compared the initial results of the dictionary-based morphological analysis model trained using the method described in Section ~\ref{sec:morphological-analysis-model} with those of MeCab and syllable-based morphological analysis systems (refer to Table~\ref{tab:performance-without-reranking}).
    The results show that the dictionary-based method implemented is superior to the existing MeCab system, but it differs from human evaluations owing to the limitations mentioned above, and it does not reach the performance of existing syllable-based morphological analysis systems.
    We also found that the compatibility between the Sejong corpus and other corpora is poor, as the model trained on the Sejong corpus has guaranteed performance when evaluated on the Sejong corpus, and some performance degradation occurs on other corpora.

    \begin{table*}[h]
        \caption{Performance comparison between morphological analysis systems without re-ranking}
        \label{tab:performance-without-reranking}
        \begin{tabular*}{500pt}{@{\extracolsep\fill}c|cccccc@{\extracolsep\fill}}
            \toprule
            \multirow{2}{*}{System}                & \multicolumn{2}{c}{Sejong}                                & \multicolumn{2}{c}{UCorpus (written)}                     & \multicolumn{2}{c}{Everyone's Corpus (written)}           \\
            \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7}
            ~                                      & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            \hspace{2mm}MeCab-ko                   & 89.17                      & 93.06                        & 87.88                      & 92.32                        & 87.77                      & 92.05                        \\
            \hspace{2mm}Syllable-based (written)   & 91.95                      & 95.16                        & \textbf{96.84}             & \textbf{97.97}               & \textbf{98.00}             & \textbf{98.82}               \\
            \hspace{2mm}Dictionary-based (written) & 90.99                      & 94.58                        & 96.33                      & 97.74                        & 96.85                      & 98.14                        \\
            \hspace{2mm}Dictionary-based (Sejong)  & \textbf{95.23}             & \textbf{97.08}               & 90.18                      & 94.19                        & 91.30                      & 94.79                        \\
            \toprule
            \multirow{2}{*}{System}                & \multicolumn{2}{c}{UCorpus (semi-spoken)}                 & \multicolumn{2}{c}{UCorpus (spoken)}                      & \multicolumn{2}{c}{Everyone's Corpus (spoken)}            \\
            \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7}
            ~                                      & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            \hspace{2mm}MeCab-ko                   & 86.85                      & 91.38                        & 81.75                      & 87.90                        & 85.28                      & 89.52                        \\
            \hspace{2mm}Syllable-based (spoken)    & \textbf{96.56}             & \textbf{97.65}               & \textbf{94.89}             & \textbf{96.76}               & \textbf{95.14}             & \textbf{96.82}               \\
            \hspace{2mm}Dictionary-based (spoken)  & 94.98                      & 96.65                        & 93.02                      & 95.71                        & 92.47                      & 94.83                        \\
            \bottomrule
        \end{tabular*}
    \end{table*}

    \begin{table*}[h]
        \caption{Performance comparison between morphological analysis systems with Two-stage re-ranking}
        \label{tab:performance-with-reranking}
        \begin{tabular*}{500pt}{@{\extracolsep\fill}c|cccccc@{\extracolsep\fill}}
            \toprule
            \multirow{2}{*}{System}                       & \multicolumn{2}{c}{Sejong}                                & \multicolumn{2}{c}{UC+EC (written)}                       & \multicolumn{2}{c}{UC+EC (spoken)}                        \\
            \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7}
            ~                                             & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            \hspace{2mm}MeCab-ko                          & 89.17                      & 93.06                        & 87.83                      & 92.19                        & 84.62                      & 89.60                        \\
            \hspace{2mm}Syllable-based                    & 91.95                      & 95.16                        & 97.42                      & 98.39                        & 95.53                      & \textbf{97.08}               \\
            \hspace{2mm}Dictionary-based (without rerank) & 95.23                      & 97.08                        & 96.59                      & 97.94                        & 93.49                      & 95.73                        \\
            \hspace{2mm}Dictionary-based (1-stage rerank) & 96.63                      & 97.84                        & 97.50                      & 98.44                        & 94.77                      & 96.62                        \\
            \hspace{2mm}Dictionary-based (2-stage rerank) & \textbf{96.87}             & \textbf{98.01}               & \textbf{97.75}             & \textbf{98.60}               & \textbf{95.56}             & \textbf{97.08}               \\
            \bottomrule
        \end{tabular*}
    \end{table*}

    \begin{table}[h]
        \centering
        \footnotesize
        \caption{Training Options and Software Information for Re-ranking Model}
        \label{tab:training-options}
        \begin{tabular}{|p{24mm}|p{25mm}|p{25mm}|}
            \hline
            ~                     & \textbf{First-Stage}                & \textbf{Second-Stage}                                        \\ \hline
            Input Type            & Only Morphological Analysis Results & First Morphological Analysis Results and Original Input Sentences \\ \hline
            Max Sequence Length   & 384                                 & 512                                                               \\ \hline
            Minibatch Size        & 120                                 & 40                                                                \\ \hline
            Training Epochs       & 5                                   & 7                                                                 \\ \hline
            Devices Used          & \multicolumn{2}{l|}{4 GPUs}                                                                             \\ \hline
            Distribution Strategy & \multicolumn{2}{l|}{ddp}                                                                                \\ \hline
            FP Precision          & \multicolumn{2}{l|}{16-bit}                                                                             \\ \hline
            Learning Rate         & \multicolumn{2}{l|}{$2 \times 10^{-5}$}                                                                 \\ \hline
            LR Scheduler          & \multicolumn{2}{l|}{ExponentialLR (gamma=0.9)}                                                          \\ \hline
            Optimizer Type        & \multicolumn{2}{l|}{AdamW}                                                                              \\ \hline
            PyTorch               & \multicolumn{2}{l|}{version 2.0.1}                                                                      \\ \hline
            PyTorch Lightning     & \multicolumn{2}{l|}{version 2.0.6}                                                                      \\ \hline
            Transformers          & \multicolumn{2}{l|}{version 4.31.0}                                                                     \\ \hline
        \end{tabular}
    \end{table}

    \subsection{Re-ranking Performance}\label{subsec:reranking-performance}

    Upon integrating the BERT-based re-ranking model, we observed substantial performance enhancement.
    Table~\ref{tab:performance-with-reranking} shows that the re-ranking model identified a better path in a significant proportion of cases.
    The first-stage re-ranking exhibited a performance improvement of over 20\% compared with traditional models.
    The subsequent re-ranking, leveraging a distinct type of input and a different pre-trained model, further augmented the performance by more than 30\%.

    \begin{table*}[]
        \caption{Comparison of performance differences with previous studies}
        \label{tab:differences-with-previous-studies}
        \begin{tabular*}{500pt}{@{\extracolsep\fill}ccc|cc@{\extracolsep\fill}}
            \toprule
            \multirow{2}{*}{Study}                & \multirow{2}{*}{Model}                          & \multirow{2}{*}{Data (train, test)}         & \multicolumn{2}{c}{Performance}                           \\
            \cmidrule{4-5}
            ~                                     & ~                                               & ~                                           & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            Na, 2015~\cite{NaSH2015}              & CRF++, Lattice-based HMM                        & Sejong 200k, 50k sentences                  & 95.22                      & 97.21                        \\
            Lee et al., 2016~\cite{LeeCH2016}     & Structural SVM                                  & Sejong 666k, 74k eojeols                    & 96.41                      & -                            \\
            Li et al., 2017~\cite{Li2017}         & Seq2seq (GRU-based)                             & Sejong 90k, 10k sentences                   & 95.33                      & 97.15                        \\
            Na and Kim, 2018~\cite{NaSH2018}      & Lattice + HMM                                   & Sejong 200k, 50k sentences                  & 96.35                      & 97.74                        \\
            Min et al., 2019~\cite{MinJW2019}     & Seq2seq (Transition-based)                      & Sejong 200k, 50k sentences                  & 96.34                      & 97.68                        \\
            Song and Park, 2019~\cite{SongHJ2019} & Seq2seq (BiLSTM-based)                          & Sejong 200k, 50k sentences                  & 95.68                      & 97.43                        \\
            Youn et al, 2021~\cite{YounJY2021}    & Seq2seq (BERT-based)                            & Sejong 675k, 75k sentences                  & 95.99                      & 97.94                        \\
            Shin et al, 2023~\cite{ShinHJ2023}    & Transformer(Encoder) + BiLSTM                   & Sejong 769k, 87k sentences                  & 96.12                      & 97.74                        \\
            \midrule
            Proposed (without rerank)             & \multirow{3}{*}{Lattice + Transformer(Encoder)} & \multirow{3}{*}{Sejong 194k, 10k sentences} & 95.23                      & 97.08                        \\
            Proposed (1-stage rerank)             & ~                                               & ~                                           & 96.63                      & 97.84                        \\
            Proposed (2-stage rerank)             & ~                                               & ~                                           & \textbf{96.87}             & \textbf{98.01}               \\
            \bottomrule
        \end{tabular*}
    \end{table*}

    Next, we performed the BERT-based re-ranking described in Section ~\ref{sec:reranking-model} and compared its performances.
    Three pre-trained language models, KPF-BERT, ETRI-ELECTRA, and ETRI-RoBERTa, which are known to perform well in other Korean language understanding tasks, were used to fine-tune the re-ranking model.
    KPF-BERT received Korean sentences as input, ETRI-ELECTRA received morphologically tagged sentences as input, and ETRI-RoBERTa receives morpheme-separated sentences as input.
    For KPF-BERT and ETRI-RoBERTa, there may be problems with model learning because of the separation of morpheme tags into letter units during the tokenization process when the morpheme analysis results were received as input and re-ranked, hence, the morpheme tags of the mid-level classification unit were added as new tokens, and then training was performed.
    As shown in Figure~\ref{fig:ranking}, for these three types of pre-trained language models, we first performed training with a re-ranking model using only the morphological analysis results and then performed training with a second re-ranking model using the top five morphological analysis results from the first re-ranking results along with the input sentences for other types of pre-trained language models.
    Preliminary tests indicate that using the same type of model or input results in a nearly identical retrained model, with no further improvement in performance.

    The sentences used for training the re-ranking model were selected from those used for training the dictionary-based morphological analysis model: 190,000 sentences from the Sejong corpus, 240,000 sentences from the written language of the combined UCorpus and Everyone's corpus, and 360,000 sentences from the spoken language of the combined UCorpus and Everyone's corpus.
    For a large number of sentences, we adopted a floating-point 16-bit technique while using four GPUs for distributed training and significantly reduced the time required for training.
    In addition, the minibatch size was 120 with a maximum sequence length of 384 because the first re-ranking uses only the morphological analysis results as input.
    The minibatch size was 40 with a maximum sequence length of 512 because the original input sentences were given as input along with the first morphological analysis results.
    See Table~\ref{tab:training-options} for information on other training options and software tools used in the implementation of re-ranking models.

    Table~\ref{tab:performance-with-reranking} shows that incorporating the re-ranking model significantly improves the performance compared with no re-ranking.
    The error reduction rate (ERR) of the performance change from the existing model on eojeol accuracy is shown as 29\%, 27\%, and 20\% for the Sejong corpus, combined written corpus, and combined spoken corpus, respectively, with the first round of re-ranking, and the second round of re-ranking improved the performance by increasing the rate to 34\%, 34\%, and 32\%, respectively.
    These performance improvements demonstrate that the dictionary-based morphological analysis model outperforms traditional syllable-based morphological analysis systems, including numerous pre- and post-processing rules and dictionaries.

    \subsection{Comparison to Other Studies}\label{subsec:comparison-to-other-studies}

    We found that the proposed transformer-based re-ranking technique consistently improved the results of the existing morphological analysis models.
    These results confirm that it opens up new possibilities by further improving the results of existing traditional machine-learning models in the field of Korean morphological analysis.
    Finally, because the major related studies proposed in the literature were mostly conducted on the Sejong corpus, we compared the performance improvement of the Sejong corpus with the results of previous studies.
    Although it is difficult to make a direct comparison because of slight differences in implementation conditions and evaluation criteria.
    The proposed dictionary-based morphological analysis model is not up to the latest research results; however, by incorporating a re-ranking model, it can secure a performance that is comparable to existing research.

    The entire morphological analysis model, including the re-ranking model, is not suitable for real-time processing.
    However, it is expected that by reflecting the cases whose ranks are changed through the re-ranking model as feedback to the dictionary-based morphological analysis model, it will be feasible to obtain near-improved morphological analysis performance.
    The improved dictionary-based morphological analysis model can then be used as input to the re-ranking model; therefore, it is expected that a gradually improvement in morphological analysis model can be obtained through this iterative feedback loop.


    \section{Related Work}\label{sec:related-work}

    Korean morphological analyses have seen an influx of various methodologies in recent years~\cite{KwonHC1991, LeeDG2009, ShimKS2011, LeeJS2011, ShinJC2012, LeeCK2013, NaSH2014, NaSH2015, HwangHS2016, KimHM2016, ChungES2016, LeeCH2016, Li2017, NaSH2018, KimSW2018, ChoiYS2018, MinJW2018, MinJW2019, KimHM2019, SongHJ2019, MinJW2020, SongHJ2020, ChoiYS2020, HwangHS2020, KimHJ2021, YounJY2021, MinJW2022, KimJM2022, ShinHJ2023}.
    The nature of the Korean language, being agglutinative, introduces challenges that have propelled researchers to develop inventive solutions, many of which have laid the groundwork for future research.
    Table~\ref{tab:overview-of-recent-korean-morphological-analysis-methods} provides a concise comparison of the methodologies and key concepts of key studies that are directly or indirectly related to this research, giving a brief overview of the different methods of morphological analysis.

    \begin{table}[h!]
        \centering
        \footnotesize % OR use \tiny, \small, \footnotesize, \scriptsize
        \caption{Overview of Recent Korean Morphological Analysis Methods}
        \label{tab:overview-of-recent-korean-morphological-analysis-methods}
        \begin{tabular}{|p{10mm}|p{24mm}|p{40mm}|}
            \hline
            \textbf{Study}                        & \textbf{Methodology}                                   & \textbf{Key Concepts}                                                                                             \\
            \hline
            Na et al., 2014~\cite{NaSH2014}       & Lattice-based Discriminative Approach                  & Lattice creation from a lexicon, morpheme connectivity, path optimization in morpheme lattice, POS tagging.       \\
            \hline
            Na, 2015~\cite{NaSH2015}              & Two-stage Discriminative Approach using CRFs           & Statistical morphological analysis, CRF-based morpheme segmentation and POS tagging, full sentence application.   \\
            \hline
            Na and Kim, 2018~\cite{NaSH2018}      & Phrase-based Model with CRFs                           & Phrase-based processing units, CRF integration for morpheme segmentation and POS tagging, noise-channel modeling. \\
            \hline
            Shim, 2011~\cite{ShimKS2011}          & Syllable-based POS Tagging with CRFs                   & Syllable-based tagging, efficiency in label assignment, morphological analysis bypass.                            \\
            \hline
            Lee, 2013~\cite{LeeCK2013}            & Joint Model with Structural SVM                        & Word spacing and POS tagging joint modeling, error propagation reduction, structural SVM application.             \\
            \hline
            Lee et al., 2016~\cite{LeeCH2016}     & Hybrid Algorithm with Pre-analyzed Dictionary          & Syllable-based POS tagging, integration of pre-analyzed dictionary and machine learning, CRF application.         \\
            \hline
            Kim et al., 2016~\cite{KimHM2016}     & POS Tagging with Bi-LSTM-CRFs                          & Syllable pattern input, bi-directional LSTM and CRF for POS tagging, morpheme ambiguity handling.                 \\
            \hline
            Li et al., 2017~\cite{Li2017}         & Sequence-to-Sequence Model with Convolutional Features & Seq2seq model with convolutional features for morphological analysis, POS tagging.                                \\
            \hline
            Kim and Choi, 2018~\cite{KimSW2018}   & Integrated Model with Bidirectional LSTM-CRF           & Bidirectional LSTM and CRF for word spacing and POS tagging, syllable-based approach.                             \\
            \hline
            Choi and Lee, 2018~\cite{ChoiYS2018}  & Reranking Model with Seq2Seq Outputs                   & Seq2Seq model reranking, morpheme-unit embedding, n-gram based morpheme reordering.                               \\
            \hline
            Min et al., 2019~\cite{MinJW2019}     & Neural Transition-based Model                          & End-to-end neural transition-based learning, morpheme segmentation, sequence-to-sequence POS tagging.             \\
            \hline
            Kim et al., 2019~\cite{KimHM2019}     & Syllable Distribution Patterns with Bi-LSTM-CRF        & Utilization of syllable distribution, Bi-LSTM-CRF for morphological analysis and POS tagging.                     \\
            \hline
            Song and Park, 2019~\cite{SongHJ2019} & Tied Sequence-to-Sequence Multi-task Model             & Multi-task learning for morpheme processing and POS tagging, pointer-generator and CRF network integration.       \\
            \hline
            Song and Park, 2020~\cite{SongHJ2020} & Two-step Korean POS Tagger with Encoder-Decoder        & Encoder-decoder architecture for morpheme generation, sequence labeling for POS tagging.                          \\
            \hline
            Youn and Lee, 2021~\cite{YounJY2021}  & Two-step Deep Learning-based Pipeline Model            & Deep learning sequence-to-sequence models, BERT for morpheme restoration and POS tagging.                         \\
            \hline
            Shin and Lee, 2023~\cite{ShinHJ2023}  & Syllable-Based Multi-POSMORPH Annotation               & Syllable distribution patterns, Multi-POSMORPH tagging, Transformer encoder, BiLSTM usage.                        \\
            \hline
        \end{tabular}
    \end{table}

    \subsection{Traditional Dictionary-based Approaches}\label{subsec:traditional-dictionary-based-approaches}
    The earliest attempts at Korean morphological analysis relied heavily on rule- and dictionary-based methods~\cite{KwonHC1991}.
    These methodologies employ predefined sets of linguistic rules or large dictionaries to detect morphemes and determine parts of speech.
    One of the most significant advantages of this approach is its deterministic nature, which can lead to high accuracy when the input text closely adheres to the rules or dictionaries used.
    However, it is challenging to scale and update them, particularly with the constant evolution of language and the emergence of new words.
    The dynamic nature of language, particularly in the Internet age, has made the maintenance of comprehensive dictionaries labor-intensive.

    \subsection{Syllable-unit Morphological Analysis}\label{subsec:syllable-unit-morphological-analysis}
    To overcome the limitations of dictionary dependence, syllable-by-syllable morphological analysis has emerged as an alternative~\cite{ShimKS2011, LeeCK2013, LeeCH2016, KimHM2016, Li2017, KimSW2018, ChoiYS2018, MinJW2019, KimHM2019, SongHJ2019, SongHJ2020, YounJY2021, ShinHJ2023}.
    This method either tags each syllable and then applies a base-form restoration dictionary~\cite{ShimKS2011, LeeCH2016} or tags the syllable with the base form already restored~\cite{YounJY2021}.
    One notable drawback is the challenge of accurately pinpointing morpheme boundaries.
    Furthermore, as the sequences increase in length, the system finds it increasingly challenging to understand long-term contextual data.

    \subsection{Recent Deep Learning Approaches}\label{subsec:recent-deep-learning-approaches}
    The incorporation of deep learning into Korean morphological analysis has brought significant advancements to the field.
    Existing deep learning methods typically employ architectures like Bidirectional Long Short-Term Memory (Bi-LSTM) networks, Convolutional Neural Networks (CNNs), and Transformer-based models.
    These approaches focus on understanding language context and sequence, utilizing the ability of these models to capture long-range dependencies and intricate patterns in text data.
    For example, Bi-LSTM-CRF models, extensively used for sequence labeling in morphological analysis, leverage LSTM's capacity to remember long-term dependencies and CRF's proficiency in sequence prediction.

    In contrast, our method innovatively integrates the re-ranking concept with BERT-based models for Korean morphological analysis.
    Unlike traditional deep learning methods that primarily use sequence-to-sequence or sequence labeling approaches, our method generates suboptimal paths using dictionary-based techniques, which are then re-ranked by BERT models.
    This dual approach, leveraging BERT's contextual understanding, allows for a more detailed and accurate morphological analysis.
    The distinction of our approach lies in its ability to address the complexities of the Korean language.
    By generating and re-ranking suboptimal paths, our method can identify and rectify anomalies that standard deep learning models may miss.
    This innovative strategy combines the precision of dictionary-based methods with the contextual comprehension of BERT models, marking a significant advancement in the field, especially for languages with intricate morphological structures like Korean.

    \subsection{Integrating Dictionary-based and Deep Learning Approaches}\label{subsec:integrating-dictionary-based-and-deep-learning-approaches}
    Tokenisation, a fundamental process in NLP deep learning models, involves breaking down text into smaller units and converting these tokens into vectors for computational processing.
    For Korean, with its complex morphological characteristics, tokenisation that respects morpheme boundaries is crucial.
    This is because it can not only accurately capture the linguistic nuances of Korean, but also improve the overall performance of deep learning models.
    This is especially important given the agglutinative nature of Korean, where words are formed by combining morphemes with different semantic and syntactic information.

    In this respect, the combination of dictionary-based morphological analysis methods and deep learning approaches used by MeCab~\cite{MeCab}, a fast and lightweight morphological analyser used for tokenisation of Korean and Japanese, is quite useful.
    The dictionary-based morphological analysis used here uses a model trained with CRFs to form a lattice structure as in ~\cite{Kudo2004, NaSH2014, NaSH2018} to identify the optimal path for morphological analysis.
    While this method provides a degree of accuracy and speed, it does not achieve the high accuracy achieved by modern deep learning.

    Our research sought to bridge this gap by effectively combining these dictionary-based morphological analysis methods with the contextual understanding capabilities of deep learning.
    Future research should continue to refine these hybrid methods to explore the possibility of end-to-end models that seamlessly combine the strengths of traditional dictionary-based analysis with the adaptive capabilities of deep learning.
    This direction promises significant advances in morphological analysis and will further push the boundaries of Korean language processing.


    \section{Conclusion}\label{sec:conclusion}

    This study signifies a progressive stride in Korean morphological analysis by seamlessly merging conventional dictionary-based techniques with advanced deep learning methodologies.
    Our findings indicate that while relying solely on dictionary-based morphological analysis does not surpass the efficacy of some existing models, the integration of a BERT-based re-ranking system notably enhances accuracy, establishing a new standard in this domain.

    While the performance improvement increases computational demand, the introduced methodology provides a promising avenue for continuous enhancement.
    This innovative amalgamation of classical dictionary approaches and cutting-edge machine-learning methodologies paves the way for groundbreaking advancements in the intricate and multifaceted domains of Korean linguistic processing.

    Future endeavors in this domain should emphasize the refinement of this harmonious integration to achieve even higher precision in morphological analysis while optimizing computational efficiency.
    Moreover, our observations indicate the potential for employing a probabilistic model to discern areas where inaccuracies are likely to arise, thus enabling the retrieval of more accurate interpretations from a narrower candidate pool.
    The parallels between this initiative and the challenges of translation quality estimation suggest that insights from the latter can bolster the efficacy of our approach.


    \bibliography{references}

\end{document}
