\documentclass[AMS,STIX2COL]{WileyNJD-v2}

\articletype{Article Type}
\received{26 April 2016}
\revised{6 June 2016}
\accepted{6 June 2016}
\raggedbottom

\begin{document}
    \title{Improvement of Korean Morphological Analysis System \\Through Transformer-based Re-ranking}

    \author[1]{Author One*}
    \author[2,3]{Author Two}
    \author[3]{Author Three}
    \authormark{AUTHOR ONE \textsc{et al}}

    \address[1]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}
    \address[2]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}
    \address[3]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}
    \corres{*Corresponding author name, This is sample corresponding address. \email{authorone@gmail.com}}
    \presentaddress{This is sample for present address text this is sample for present address text}

    \abstract[Abstract]{Korean morphological analysis plays a basic and important role enough to be called the first step in Korean language analysis. Due to the nature of Korean agglutinative words, it was difficult to build an automatic analysis system because the analysis was not completed with part-of-speech tagging alone, as in English. In addition, various methods for morphological analysis have been proposed, but efficient methods such as BPE are mainly used in applications intended to be used as tokenizers for deep learning. In this paper, we propose a method to maximize the performance of an efficient morphological analysis system that can be used for tokenization through multi-stage re-ranking based on deep learning. For a number of various cases whose rankings have been changed through re-ranking, it is possible to improve performance while maintaining speed by updating the cost matrix of the lattice-based morphological analysis system in the future. Through experiments, we showed that the proposed method effectively improved performance by reducing errors by more than 30\% in both the spoken language model and the written language model.}
    \keywords{Korean morphological analysis, natural language understanding, deep learing, pretrained transformer encoder, re-ranking}

    \JELinfo{classification}
    \MSC{Code numbers}
    \jnlcitation{\cname{
        \author{Williams K.},
        \author{B. Hoskins},
        \author{R. Lee},
        \author{G. Masato}, and \author{T. Woollings}} (\cyear{2016}),
        \ctitle{A regime analysis of Atlantic winter jet variability applied to evaluate HadGEM3-GC2}, \cjournal{Q.J.R. Meteorol. Soc.}, \cvol{2017;00:1--6}.}
    \maketitle
    \footnotetext{\textbf{Abbreviations:} ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor}


    \begin{center}
        \begin{table*}[t]
            \caption{Maximum performance of alternative paths as correct answers}\label{tab:maximum-performance}
            \centering
            \begin{tabular*}{500pt}{@{\extracolsep\fill}cccD{.}{.}{3}c@{\extracolsep\fill}}
                \toprule
                & \multicolumn{2}{@{}c@{}}{\textbf{Written Language Evaluation Set}} & \multicolumn{2}{@{}c@{}}{\textbf{Spoken Language Evaluation Set}} \\\cmidrule{2-3}\cmidrule{4-5}
                alternative range & eojeol accuracy & average number of alternative & \multicolumn{1}{@{}l@{}}{eojeol accuracy}  & average number of alternative \\
                \midrule
                no alternative    & 96.36           & 1.0                           & 92.54                                     & 1.0                           \\
                secondary         & 98.74           & 25.7                          & 97.27                                     & 12.9                          \\
                tertiary          & 98.96           & 47.8                          & 97.81                                     & 23.6                          \\
                quarternary       & 99.01           & 69.6                          & 97.95                                     & 34.2                          \\
                quinary           & 99.02           & 91.1                          & 98.01                                     & 44.5                          \\
                \bottomrule
            \end{tabular*}
            \begin{tablenotes}
                \footnotesize
                \item\hspace{2mm} * Written Language Evaluation Set: 2,400 sentences each randomized from UCorpus and Everyone Corpus (4,800 sentences total)
                \item\hspace{2mm} * Spoken Language Evaluation Set: 2,400 sentences each randomized from UCorpus and Everyone Corpus (4,800 sentences total)
            \end{tablenotes}
        \end{table*}
    \end{center}


    \section{Introduction}\label{sec:intro}

    Korean morphological analysis is the process of determining parts of speech by finding morphemes, which are the smallest units of linguistic expression that have an independent meaning in a sentence.
    In an isolating language like English, this can be done relatively easily by tagging parts of speech sequentially, but in Korean, the nature of the agglutinative language requires separating endings or investigatives and restoring inflections to their original form.
    In addition, since the basic input of other Korean analysis tasks is often a separated morpheme, the accuracy of morphological analysis greatly affects the performance of Korean analysis.
    Modern high-performance deep learning methods in natural language processing use a tokenization process that breaks text into smaller units, and then converts each token into a vector as an input to the computational model~\cite{Mikolov2013}.
    In this case, the token unit is mainly subword units, and to reflect the characteristics of Korean, subword tokenization with separated morphemes is attempted in advance~\cite{SongHJ2021}.
    Using the results of morphological analysis for this tokenization process improves the overall performance of the analysis by reflecting the semantic units of Korean, but requires a highly accurate and fast morphological analyzer.

    Various approaches have been proposed for morphological analysis, which plays a fundamental and important role in Korean language analysis~\cite{KwonHC1991, LeeDG2009, ShimKS2011, LeeJS2011, ShinJC2012, LeeCK2013, NaSH2014, NaSH2015, HwangHS2016, KimHM2016, ChungES2016, LeeCH2016, Li2017, NaSH2018, KimSW2018, ChoiYS2018, MinJW2018, MinJW2019, KimHM2019, SongHJ2019, MinJW2020, SongHJ2020, ChoiYS2020, HwangHS2020, KimHJ2021, YounJY2021, MinJW2022, KimJM2022, ShinHJ2023}.
    In general, when people understand speech or writing, they try to make sense of it using the vocabulary and concepts they know.
    While there are ways to use rules or dictionaries to reflect this way of understanding~\cite{KwonHC1991}, the problem is that it becomes difficult to build and maintain a dictionary for vocabulary that appears in every text.
    Therefore, methods for tagging syllable units without a dictionary have been proposed~\cite{ShimKS2011, LeeCK2013, LeeCH2016, KimHM2016}, and studies to improve them have been carried out continuously~\cite{KimSW2018, ChoiYS2018, KimHM2019, MinJW2019, SongHJ2019, SongHJ2020, YounJY2021, ShinHJ2023}.
    From a mechanical point of view, syllable-by-syllable morphological analysis can be done either by tagging syllable-by-syllable and then applying a base form restoration dictionary~\cite{ShimKS2011, LeeCH2016}, or by tagging syllable-by-syllable with the base form already restored~\cite{YounJY2021}.
    However, syllable-by-syllable morphological analysis has limitations in that it is difficult to accurately identify morpheme boundaries, and it is difficult to learn long-term contextual information as the length of the sequence increases.
    In this paper, the former is referred to as dictionary-based morphological analysis, and the latter is referred to as syllable-unit morphological analysis.
    Both methods also share the limitation that they are trained on a manually labeled corpus and cannot accurately analyze new syllable combinations or morphemes that do not appear in the training corpus.
    In recent years, with the development of the Internet and the spread of open source and open data, web texts, corpora, language resources, and knowledge shared by different people have accumulated significantly.
    The reduced cost of building and maintaining a dictionary can be a great opportunity to overcome the limitations of dictionary-based methods.

    Against this background, this paper considers how the dictionary-based morphological analysis method used by MeCab~\cite{MeCab}, an open software for Korean and Japanese morphological analysis in tokenizers, an essential preprocessing tool for deep learning, can be effectively improved by deep learning, and proposes a method.
    The dictionary-based morphological analysis method~\cite{Kudo2004, NaSH2014, NaSH2015, NaSH2018} trained by the CRF method~\cite{Lafferty2001} lists the candidate morphemes in the dictionary from a given sentence to form a lattice structure connected by a directed graph, and finds the optimal morphological analysis path within it.
    The process of finding the best path in the lattice uses the Viterbi algorithm~\cite{Viterbi1967}, which finds the path that minimizes the cost of each morpheme node and the sum of the neighborhood costs of two consecutive morphemes.
    The main types of errors in these dictionary-based morphological analysis methods are when new words not in the dictionary are used in a sentence, or when the optimal path calculation selects the wrong result due to a bias.
    For example, it may be less costly to select one long morpheme than several short morphemes, but it may sometimes be the wrong analysis.
    The main motivation for this research is that the path that minimizes the sum of all costs for nodes and links may not be the optimal path.

    In order to identify the cases where a suboptimal solution is actually the best solution according to the best path calculation, we modified the best path calculation method to generate suboptimal analysis results and check the extent to which they are correct.
    While there are many different ways to choose the next best path, we used the method of replacing a morpheme node on the optimal path with a lower ranked node.
    As shown in Table~\ref{tab:maximum-performance}, we were able to confirm the extent to which analysis performance could be improved by replacing the optimal path with a lower-ranked node.
    We can think of the problem of finding the actual correct answer among these generated suboptimals as similar to the problem of re-ranking search results in information retrieval~\cite{BaeYJ2021}.
    Previously, in ~\cite{ChoiYS2018}, the N-best analysis results generated by the seq2seq model were re-ranked based on a convolutional neural network to improve performance.
    In this study, the re-ranking was performed using two BERT models of different types and forms as proposed in ~\cite{Nogueira2019}.
    The experimental results show that the first stage re-ranking improves the performance by more than 20\% over the previous written and spoken models, and the second stage re-ranking with a different type of input and a different type of pre-trained model further improves the performance by more than 30\% over the previous written and spoken models.

    With this method, the performance of the dictionary-based morphological analysis method could be further improved, but the overall analysis time increases when the morphological analysis system is configured including the re-ranking model itself.
    However, it is possible to use the results of multiple re-ranked morpheme analyses to update the connection costs between morphemes in the dictionary, similar to the backpropagation process in a typical neural network.
    It is also expected that the morphological analysis system with improved connection costs will be able to generate better re-ranking candidates, which will further improve performance by doing so iteratively.
    Further research is needed for this in the future, and in this paper, only the performance improvement through the second stage re-ranking was covered as the scope of the study.
    The main contributions of this study are as follows:
    \begin{enumerate}
        \item \textbf{Further improvement of dictionary-based morphological analysis method using suboptimal analysis results}: We explore the possibility of performance improvement by introducing a method to replace the optimal path with a suboptimal node, and propose a method to effectively improve the dictionary-based morphological analysis method through deep learning.
        \item \textbf{Extending the performance improvement by introducing a two-stage re-ranking model}: To improve the performance of dictionary-based analysis by re-ranking the morphological analysis results, we propose to extend the performance improvement by using different BERT models to perform two rounds of re-ranking.
        \item \textbf{A method for updating connection costs in the dictionary and suggestions for future research}: We propose a new method for updating connection costs in dictionaries based on the results of reranked morphological analysis, and provide directions for future research by suggesting improvements and directions for future research.
    \end{enumerate}
    These contributions provide important insights into the performance improvement of Korean morphological analysis and the direction of future research, and will serve as a useful reference for future researchers.

    This paper is organized as follows:
    In Section ~\ref{sec:related-work}, we introduce previous research cases related to this study, and in Section ~\ref{sec:morphological-analysis-model}, we discuss how to configure and train a dictionary-based morphological analysis system.
    In Section ~\ref{sec:re-ranking-model}, we discuss how to generate secondary results of morphological analysis and produce re-ranking data, and propose a method for training a two-stage re-ranking model.
    In Section ~\ref{sec:results}, we discuss the results of performance improvement by morphological analysis model and re-ranking model.
    Finally, in Section ~\ref{sec:conclusion}, we conclude the paper and discuss the limitations of this study and the direction of future research.


    \begin{figure*}[t]
        \centerline{\includegraphics[width=0.75\textwidth]{fig1.1}}
        \caption{Transformation of a single sentence in the Korean morpheme-tagged corpus into a single training sample}\label{fig:sample}
    \end{figure*}


    \section{Related Work}\label{sec:related-work}

    Korean morphological analysis has seen an influx of various methodologies over the years~\cite{KwonHC1991, LeeDG2009, ShimKS2011, LeeJS2011, ShinJC2012, LeeCK2013, NaSH2014, NaSH2015, HwangHS2016, KimHM2016, ChungES2016, LeeCH2016, Li2017, NaSH2018, KimSW2018, ChoiYS2018, MinJW2018, MinJW2019, KimHM2019, SongHJ2019, MinJW2020, SongHJ2020, ChoiYS2020, HwangHS2020, KimHJ2021, YounJY2021, MinJW2022, KimJM2022, ShinHJ2023}. The nature of the Korean language, being agglutinative, introduces challenges that have propelled researchers to come up with inventive solutions, many of which have laid the groundwork for future research.

    \subsection{Traditional Dictionary-based Approaches}\label{subsec2.1}
    The earliest attempts at Korean morphological analysis heavily depended on rule-based systems and dictionary-based methods~\cite{KwonHC1991}. These methodologies essentially employed predefined sets of linguistic rules or large dictionaries to detect morphemes and determine parts of speech. One of the most significant advantages of such an approach is its deterministic nature, which can lead to high accuracy when the input text closely adheres to the rules or dictionaries used. However, they are notably challenging to scale and update, particularly with the constant evolution of the language and emergence of new words. The dynamic nature of language, especially in the age of the internet, has made maintaining comprehensive dictionaries labor-intensive.

    \subsection{Syllable-unit Morphological Analysis}\label{subsec2.2}
    In an attempt to bypass the limitations of dictionary-dependence, syllable-by-syllable morphological analysis emerged as an alternative~\cite{ShimKS2011, LeeCK2013, LeeCH2016, KimHM2016, KimSW2018, ChoiYS2018, KimHM2019, MinJW2019, SongHJ2019, SongHJ2020, YounJY2021, ShinHJ2023}. This method either tags each syllable and then applies a base form restoration dictionary~\cite{ShimKS2011, LeeCH2016} or tags the syllable with the base form already restored~\cite{YounJY2021}. One notable drawback is the challenge in accurately pinpointing morpheme boundaries. Furthermore, as sequences increase in length, the system finds it increasingly challenging to understand long-term contextual data.

    \subsection{Deep Learning and Tokenization}\label{subsec2.3}
    The advent of deep learning has reshaped the landscape of Korean morphological analysis. High-performance deep learning models in NLP now employ tokenization, dividing the text into smaller units and then converting these tokens into vectors as an input to the computational model~\cite{Mikolov2013}. Especially significant is the attempt to conduct subword tokenization, incorporating separated morphemes to cater to Korean linguistic characteristics. In particular, it is noteworthy that we attempted to tokenize subwords by incorporating segmented morphemes to suit the linguistic characteristics of Korean, and it is known that incorporating accurate morpheme analysis results into tokenization can improve overall analysis performance.

    However, with the progression of technology and accessibility to vast amounts of data, the possibility of integrating dictionary-based methods with deep learning techniques emerged. As motivated in our study, MeCab~\cite{MeCab}, an open software for Korean and Japanese morphological analysis, utilized dictionary-based morphological analysis trained by the CRF method~\cite{Lafferty2001}. This method attempts to find the optimal morphological analysis path by forming a lattice structure as in ~\cite{Kudo2004, NaSH2014, NaSH2015, NaSH2018}. The amalgamation of dictionary-based methods with deep learning, as explored in our research, signifies the latest stride in this journey, aiming to harness the strengths of both approaches. Future research avenues include refining these hybrid methods and further exploring the capabilities of end-to-end models for this complex task.


    \begin{figure*}[t]
        \centerline{\includegraphics[width=0.9\textwidth]{fig2.1}}
        \caption{Example of lattice construction and decoding result and secondary path generation}\label{fig:lattice}
    \end{figure*}


    \section{Morphological Analysis Model}\label{sec:morphological-analysis-model}

    \subsection{Korean Morphological Analysis Corpora}\label{subsec3.1}

    A representative corpus for training Korean morphological analysis models is the Sejong Morphological Analysis Corpus, often referred to as the Sejong Corpus. The Sejong Corpus, a major output of the 21st Century Sejong Project, contains 15 million words and remains a mainstay of Korean morphological analysis research~\cite{ChoeMW2008}.

    Subsequent efforts have extended and refined this foundational work. The University of Ulsan, addressing the limitations of the Sejong corpus~\cite{KimIH2010}, introduced [UCorpus], an expanded corpus of 63 million words. This expanded corpus not only added new data, but also corrected errors in the original Sejong corpus~\cite{UCorpusHG}. To take this a step further, the National Institute of the Korean Language launched a massive Korean corpus construction project in 2018. The fruits of this labor, a corpus called <Everyone Corpus>, were released to the public in 2020~\cite{EveryoneCorpus}. A special offshoot of this project produced a morpheme-tagged corpus of 3 million words, including spoken language~\cite{KimIH2019}.

    \subsection{Training Example Transformation}\label{subsec3.2}

    In order to effectively train a dictionary-based morpheme analysis model, the morpheme-tagged corpus, typically represented in lemma form, needs to be transformed to include boundary information between morphemes in its surface form. Crucial to this transformation is string alignment, a process that accounts for discrepancies between lemma and surface forms in the Korean morphological analysis corpus.

    In this research, string alignment was performed using the Smith-Waterman algorithm, which uses a scoring matrix based on the similarity of the grapheme unit of Korean letters for each word pair (as shown in Figure~\ref{fig:sample}). Each aligned sentence containing a morpheme tag can be converted into a training sample tailored for dictionary-based morphological analysis.

    The resulting table data in Figure~\ref{fig:sample} illustrates this process. Here, each row acts as a lexical unit. The first four columns contribute to feature generation, while the last four columns facilitate post-lemmatization. Using the above morphological corpus, a large number of training samples can be generated according to the process shown in Figure~\ref{fig:sample}. Except for the evaluation samples, the remaining sentences are used to train the dictionary-based morphological analysis model via the CRF algorithm. The output of this training allows the calculation of the costs associated with each morpheme node and the linking of two consecutive morphemes. This in turn allows the discovery of the optimal path using the Viterbi algorithm.

    \begin{figure*}[t]
        \centerline{\includegraphics[width=0.9\textwidth]{fig3.0}}
        \caption{Two-stage re-ranking model for Korean morphological analysis}\label{fig:ranking}
    \end{figure*}

    \subsection{Lattice Construction and Decoding}\label{subsec3.3}

    Figure~\ref{fig:lattice} shows a snapshot of the lattice structure, an integral part of morphological analysis. Part (1) shows a fragment of the lattice structure formed when the example sentence from Figure~\ref{fig:sample} is entered. Meanwhile, part (2) shows the optimal path as determined by the Viterbi algorithm.

    However, the path inferred by the trained model may differ from the correct solution constructed by a human. The nodes marked with stars in part (1) represent the correct nodes. The upper left number of each node indicates the ranking of the nodes accessible at each decoding point. It can be seen that the choices made at certain moments can deviate from the correct solution. In order to improve the performance of the analysis, it is imperative to develop mechanisms to correct these discrepancies.


    \section{Re-ranking Model}\label{sec:re-ranking-model}

    \subsection{Motivation and Background}\label{subsec4.1}

    While dictionary-based morphological analysis offers significant advances in Korean language processing, its optimal paths occasionally diverge from the correct solutions that humans understand. This divergence underscores the need for a model that re-evaluates these primary results and reorients them to achieve higher accuracy. This method, called "re-ranking," involves generating multiple analyses of an input and then reordering them based on a new set of criteria or models, thereby improving the overall quality of the results.

    \subsection{Secondary Path Generation}\label{subsec4.2}

    Before re-ranking can begin, multiple analyses, typically referred to as N-best paths, of the input sentence are generated.
    This involves extracting the top N analyses from the lattice structure in general.
    In this study, a novel approach has been introduced to generate secondary paths, as shown in part (3) of Figure~\ref{fig:lattice}, by selecting the second-best node instead of each best node constituting the path from the best-path result.
    Encouragingly, some of these secondary paths provided alternatives that reconciled incorrect answers with the correct ones.
    Similarly, paths modified by favoring the third-best node were called tertiary paths, and this naming convention continued for subsequent paths.
    In our preliminary test, the secondary paths, including the optimal and some suboptimal paths, were shown to cover the majority of correct morphological analyses as measured by human judgments. (See Table~\ref{tab:maximum-performance})

    \subsection{BERT-based Re-ranking}\label{subsec4.3}

    BERT (Bidirectional Encoder to Transformer Representation) models~\cite{Devlin2019} have revolutionized many natural language processing tasks by understanding the context in which words appear in text. In this study, we attempt to leverage the power of BERT to reorder the generated secondary paths. We labeled the generated secondary paths with scores related to morphological analysis performance in order to fine-tune a pre-trained BERT model specialized for Korean with a large amount of Korean text. After pre-testing several scoring methods on a modest scale, we found that using scores based on the degree of error, rather than scores based on accuracy, to widen the gap between correct and incorrect answers is effective for learning.

    Once the BERT model has been fine-tuned and trained for the reranking task, it will be able to predict a reranking score for each path in the secondary path list. This means that, taking into account the context, morphological organization, and other essential linguistic features of the path, the model will assign a score for each path. The paths are then re-ranked according to this score, and the path with the highest score is selected as the best morphological analysis result.

    \begin{table*}[]
        \caption{Statistics on full data for training and evaluating three morphological analysis models}\label{tab:data-statistics}
        \centering
        \begin{tabular*}{500pt}{@{\extracolsep\fill}|c|c|rrr|rrr|@{\extracolsep\fill}}
            \toprule
            \multicolumn{2}{|c|}{\multirow{2}{*}{\textbf{Corpus}}} & \multicolumn{3}{c|}{\textbf{Training Set}} & \multicolumn{3}{c|}{\textbf{Test Set}} \\
            \cmidrule{3-5} \cmidrule{6-8}
            \multicolumn{2}{|c|}{~} & sentences & eojeols & morphemes & sentences & eojeols & morphemes \\
            \midrule
            \multicolumn{2}{|c|}{Sejong Corpus} & 194,822 & 2,681,582 & 6,033,785 & 49,922 & 678,578 & 1,527,803 \\
            \midrule
            \multirow{3}{*}{UCorpus}         & written     & 4,998,560 & 57,393,332 & 127,171,473 & 53,003 & 598,413 & 1,325,419 \\
            ~                                & semi-spoken & 334,061   & 2,960,146  & 6,468,494   & 38,960 & 332,285 & 726,398   \\
            ~                                & spoken      & 429,215   & 2,295,940  & 5,584,494   & 62,399 & 279,545 & 691,542   \\
            \midrule
            \multirow{2}{*}{Everyone Corpus} & written     & 129,352   & 1,713,367  & 3,944,118   & 14,442 & 191,223 & 440,052   \\
            ~                                & spoken      & 137,869   & 714,021    & 1,440,746   & 19,789 & 85,316  & 170,523   \\
            \bottomrule
        \end{tabular*}
    \end{table*}

    \subsection{Two-stage Re-ranking}\label{subsec4.4}

    Given the complexity of the Korean language, a single reranking step does not always yield the most accurate results. Therefore, we propose a two-step reranking approach, as in ~\cite{Nogueira2019}.

    In the first step, we re-rank the secondary paths generated using the BERT model as described in section ~\ref{subsec4.3}. In the second step, we introduce another BERT variant that is optimized for a different set of linguistic features or trained on a different dataset. This allows us to perform a fine-grained re-evaluation, further refining the list and pushing the more contextually accurate paths to the top.

    As shown in Figure~\ref{fig:ranking}, for a two-stage reranking model, the first stage performs a first reranking, taking as input a secondary path in morphologically tagged lemma form. It then performs a second reranking, again taking as input the path reranked in stage 1 and the original input sentence. This is done to improve the effectiveness, since it didn't work very well when given the same type of input.

    \subsection{Iterative Updatability}\label{subsec4.5}

    The re-ranked paths are then integrated with the original dictionary-based morphological analysis. The connection costs between morphemes in the dictionary are updated based on feedback from the re-ranking model. Over time, this feedback loop ensures that the morphological analysis model becomes more closely aligned with human understanding and minimizes discrepancies in optimal path selection.


    \section{Experimental Results}\label{sec:results}

    In our effort to enhance Korean morphological analysis, we evaluated the performance of the proposed deep learning integrated dictionary-based morphological analysis method. This section presents the results of the experimental evaluation, taking into account the improvement over conventional methods and the effectiveness of our re-ranking model.

    \subsection{Setup and Data}\label{subsec5.1}

    For our experiments, we used the Sejong corpus (the version used in ~\cite{NaSH2014, NaSH2015, NaSH2018, SongHJ2019, SongHJ2020}), UCorpus\cite{UCorpusHG}, and Everyone Corpus\cite{EveryoneCorpus}. For comparison with previous studies, the Sejong corpus was trained with a single model without separation. UCorpus and Everyone Corpus provided separate spoken corpus consisting of drama scripts and broadcast dialogues, while UCorpus separated documents that were thought to be close to spoken language and further organized them into a semi-spoken corpus. Because UCorpus and Everyone Corpus have synergistic effects when trained together, we trained the models separately for written and spoken language, rather than separating them by source. The statistics of the full data for the three types of models are presented in Table~\ref{tab:data-statistics}, and due to the large volume of UCorpus, we randomly selected some of them for the actual model training.

    We transformed this organized morphological corpus with the training example transformation process described in section ~\ref{subsec3.2} to generate samples for training a dictionary-based morphological analysis model.

    \begin{table*}[]
        \caption{Performance comparison between morphological analysis systems without Re-ranking}\label{tab:performance-without-reranking}
        \begin{tabular*}{500pt}{@{\extracolsep\fill}c|cccccc@{\extracolsep\fill}}
            \toprule
            \multirow{2}{*}{System} & \multicolumn{2}{c}{Sejong} & \multicolumn{2}{c}{UCorpus (written)} & \multicolumn{2}{c}{Everyone Corpus (written)} \\
            \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7}
            & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            \hspace{2mm}MeCab-ko                   & 89.17                      & 93.06                        & 87.88                      & 92.32                        & 87.77                      & 92.05                        \\
            \hspace{2mm}Syllable-based (written)   & 91.95                      & 95.16                        & \textbf{96.84}             & \textbf{97.97}               & \textbf{98.00}             & \textbf{98.82}               \\
            \hspace{2mm}Dictionary-based (written) & 90.99                      & 94.58                        & 96.33                      & 97.74                        & 96.85                      & 98.14                        \\
            \hspace{2mm}Dictionary-based (Sejong)  & \textbf{95.23}             & \textbf{97.08}               & 90.18                      & 94.19                        & 91.30                      & 94.79                        \\
            \toprule
            \multirow{2}{*}{System} & \multicolumn{2}{c}{UCorpus (semi-spoken)} & \multicolumn{2}{c}{UCorpus (spoken)} & \multicolumn{2}{c}{Everyone Corpus (spoken)} \\
            \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7}
            & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            \hspace{2mm}MeCab-ko                   & 86.85                      & 91.38                        & 81.75                      & 87.90                        & 85.28                      & 89.52                        \\
            \hspace{2mm}Syllable-based (spoken)    & \textbf{96.56}             & \textbf{97.65}               & \textbf{94.89}             & \textbf{96.76}               & \textbf{95.14}             & \textbf{96.82}               \\
            \hspace{2mm}Dictionary-based (spoken)  & 94.98                      & 96.65                        & 93.02                      & 95.71                        & 92.47                      & 94.83                        \\
            \bottomrule
        \end{tabular*}
    \end{table*}

    \subsection{Evaluation Metrics}\label{subsec5.2}

    To measure the accuracy of the morphological analysis model, the correctness of the N-Best path, and the ranking accuracy of the reranking model, we used eojeol accuracy and morpheme F1 scores as evaluation metrics. To verify that they actually produced correct morphological analysis results, we measured the degree of agreement with human annotation on aforementioned corpus. However, due to the slightly different criteria and style of the annotators who labeled the different kinds of corpora, including the comparison with the MeCab-ko system, the following adjustments were made:
    \begin{itemize}
        \item Sentences containing unanalyzable tags (NF, NA, NV) were excluded from both training and evaluation.
        \item As for the tagsets, we excluded 3 unanalyzable tags from the 45 Sejong tagsets and used 42 tagsets.
        \item Each tag output by the MeCab-ko system was converted to the corresponding tag in the Sejong tagset.
        \item Chinese characters were converted to Chinese character tags (SH) even if they were semantically used as nouns, and consecutive Chinese characters were converted to a single morpheme.
        \item Similarly, symbol, numeral, ending, and postposition in the same tag were converted to a single morpheme, and decimal expressions were treated as a single morpheme, including the midpoint and the numbers before and after.
        \item If the first lemma letter of the ending is `[eo]', `[yeo]', or `[ah]', it is unified as `[eo]', and if it is `[eot]', `[yeo]', or `[ass]', it is unified as `[eot]'. %`어[eo]', `여[yeo]', or `아[ah]' % `었[eot]', `였[yeo]', or `았[ass]'
        \item Root tags (XR) used alone without affixes were replaced with common nouns (NNG) because they are mainly used in the Sejong corpus.
        \item As mentioned in ~\cite{KimIH2010}, connective endings (EC) and sentence-closing endings (EF) are not clearly defined in the tagging guidelines, and there are cases where they are used interchangeably in the corpus, so we evaluated them without distinguishing them.
        \item The distinction between `[geot]' and `[geo]' is unclear in the tagging guidelines, and there are cases where they are used interchangeably in the corpus, so we did not distinguish between them. %`것[geot]' and `거[geo]'
        \item Compound words can be interpreted as a single morpheme or as a combination of two or more morphemes or affixes, so we evaluated them without distinguishing between them.
        \item Proper nouns can also be interpreted as common nouns depending on the point of view or perspective, and human annotators have slightly different standards, so they were also evaluated without distinguishing them.
    \end{itemize}

    \subsection{Basic Performance}\label{subsec5.3}

    First, we compared the initial results of the dictionary-based morphological analysis model trained with the method described in Section ~\ref{sec:morphological-analysis-model} with MeCab and syllable-based morphological analysis systems. (See Table~\ref{tab:performance-without-reranking}) The results show that the dictionary-based method we implemented is better than the existing MeCab system, but it is different from human judgment due to the limitations mentioned above, and it does not reach the performance of existing syllable-based morphological analysis systems. We also found that the compatibility between the Sejong corpus and other corpora is not good, as the model trained on the Sejong corpus has guaranteed performance when evaluated on the Sejong corpus, and some performance degradation occurs on other corpora.

    \begin{table*}[]
        \caption{Performance comparison between morphological analysis systems with Two-staage Re-ranking}\label{tab:performance-with-reranking}
        \begin{tabular*}{500pt}{@{\extracolsep\fill}c|cccccc@{\extracolsep\fill}}
            \toprule
            \multirow{2}{*}{System} & \multicolumn{2}{c}{Sejong} & \multicolumn{2}{c}{UC+EC (written)} & \multicolumn{2}{c}{UC+EC (spoken)} \\
            \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7}
            & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            \hspace{2mm}MeCab-ko                          & 89.17                      & 93.06                        & 87.83                      & 92.19                        & 84.62                      & 89.60                        \\
            \hspace{2mm}Syllable-based                    & 91.95                      & 95.16                        & 97.42                      & 98.39                        & 95.53                      & \textbf{97.08}               \\
            \hspace{2mm}Dictionary-based (without rerank) & 95.23                      & 97.08                        & 96.59                      & 97.94                        & 93.49                      & 95.73                        \\
            \hspace{2mm}Dictionary-based (1-stage rerank) & 96.63                      & 97.84                        & 97.50                      & 98.44                        & 94.77                      & 96.62                        \\
            \hspace{2mm}Dictionary-based (2-stage rerank) & \textbf{96.87}             & \textbf{98.01}               & \textbf{97.75}             & \textbf{98.60}               & \textbf{95.56}             & \textbf{97.08}               \\
            \bottomrule
        \end{tabular*}
    \end{table*}

    \subsection{Re-ranking Performance}\label{subsec5.4}

    Upon integrating the BERT-based re-ranking model, we observed a substantial enhancement in performance. Table~\ref{tab:maximum-performance} demonstrated that the re-ranking model was able to identify a better path in a significant proportion of cases. The first-stage re-ranking itself exhibited a performance improvement of over 20\% compared to traditional models. The subsequent re-ranking, leveraging a distinct type of input and a different pre-trained model, further augmented the performance by over 30\%.

    Next, we performed the BERT-based re-ranking described in Section ~\ref{sec:re-ranking-model} and compared the performance.
    Three pre-trained language models, KPF-BERT, ETRI-ELECTRA, and ETRI-RoBERTa, which are known to perform well on other Korean language understanding tasks, were used to fine-tune the re-ranking model. KPF-BERT receives Korean sentences as input, ETRI-ELECTRA receives morphological tagged sentence as input, and ETRI-RoBERTa receives morpheme-separated sentences as input. In the case of KPF-BERT and ETRI-RoBERTa, there may be problems with model learning due to the separation of morpheme tags into letter units during the tokenization process when the morpheme analysis results are received as input and reranked, so the morpheme tags of the mid-level classification unit were added as new tokens, and then training was performed.
    As shown in Figure~\ref{fig:ranking}, for these three types of pre-trained language models, we first performed training with a re-ranking model using only the morphological analysis results, and then we performed training with a second re-ranking model using the top five morphological analysis results from the first re-ranking results along with the input sentences for other types of pre-trained language models. This is because preliminary tests have shown that given the same kind of model or the same kind of input, a model that is almost identical to the previously trained model is trained and performance is no longer improved.

    The sentences used for training the re-ranking model were selected from those used for training the dictionary-based morphological analysis model: 190,000 sentences from the Sejong corpus, 240,000 sentences from the written language of the combined UCorpus and Everyone corpus, and 360,000 sentences from the spoken language of the combined UCorpus and Everyone corpus. For the large number of sentences, we adopt a floating-point 16-bit technique while using four GPUs for distributed training, and were able to significantly reduce the time required to train.
    In addition, the minibatch size was 120 with a max sequence length of 384 because the first reranking uses only the morphological analysis results as input, and the minibatch size was 40 with a max sequence length of 512 because the original input sentences are given as input along with the first morphological analysis results. And the learning rate was set to $2 \times 10^{-5}$ and AdamW is used as an optimization algorithm.

    Table~\ref{tab:performance-with-reranking} shows that incorporating the reranking model significantly improves performance compared to no reranking. The error reduction rate (ERR) of the performance change from the existing model on eojeol accuracy is shown as 29\%, 27\%, and 20\% for the Sejong corpus, the combined written corpus, and the combined spoken corpus, respectively, with the first round of reranking, and the second round of reranking improved the performance more by increasing the rate to 34\%, 34\%, and 32\%, respectively.
    These performance improvements show that the dictionary-based morphological analysis model outperforms traditional syllable-based morphological analysis systems, including many pre- and post-processing rule and dictionaries.

    \begin{table*}[]
        \caption{Comparison of performance differences with previous studies}\label{tab:differences-with-previous-studies}
        \begin{tabular*}{500pt}{@{\extracolsep\fill}ccc|cc@{\extracolsep\fill}}
            \toprule
            \multirow{2}{*}{Authors} & \multirow{2}{*}{Model} & \multirow{2}{*}{Data (train, test)} & \multicolumn{2}{c}{Performance} \\
            \cmidrule{4-5}
            ~                                     & ~                                          & ~                                           & \multicolumn{1}{c}{eojeol} & \multicolumn{1}{c}{morpheme} \\
            \midrule
            Na, 2015~\cite{NaSH2015}              & CRF++, HMM                                 & Sejong 200k, 50k sentences                  & 95.22                      & 97.21                        \\
            Lee et al., 2016~\cite{LeeCH2016}     & Structural SVM                             & Sejong 666k, 74k eojeols                    & 96.41                      & -                            \\
            Li et al., 2017~\cite{Li2017}         & Seq2seq (GRU-based)                        & Sejong 90k, 10k sentences                   & 95.33                      & 97.15                        \\
            Na and Kim, 2018~\cite{NaSH2018}      & Lattice + HMM                              & Sejong 200k, 50k sentences                  & 96.35                      & 97.74                        \\
            Min et al., 2019~\cite{MinJW2019}     & Seq2seq (Transition-based)                 & Sejong 200k, 50k sentences                  & 96.34                      & 97.68                        \\
            Song and Park, 2019~\cite{SongHJ2019} & Seq2seq (BiLSTM-based)                     & Sejong 200k, 50k sentences                  & 95.68                      & 97.43                        \\
            Youn et al, 2021~\cite{YounJY2021}    & Seq2seq (BERT-based)                       & Sejong 675k, 75k sentences                  & 95.99                      & 97.94                        \\
            Shin et al, 2023~\cite{ShinHJ2023}    & Transformer(En) + BiLSTM                   & Sejong 769k, 87k sentences                  & 96.12                      & 97.74                        \\
            \midrule
            Proposed (without rerank)             & \multirow{3}{*}{Lattice + Transformer(En)} & \multirow{3}{*}{Sejong 194k, 10k sentences} & 95.23                      & 97.08                        \\
            Proposed (1-stage rerank)             & ~                                          & ~                                           & 96.63                      & 97.84                        \\
            Proposed (2-stage rerank)             & ~                                          & ~                                           & \textbf{96.87}             & \textbf{98.01}               \\
            \bottomrule
        \end{tabular*}
    \end{table*}

    \subsection{Comparison to Other Studies}\label{subsec5.5}

    We found that the proposed transformer-based reranking technique consistently improves the results of existing morphological analysis models. These results open up new possibilities in the area of Korean morphological analysis by further improving the results of existing traditional machine learning models.
    Finally, since the results of the major related works proposed in the literature are mostly conducted on the Sejong corpus, we would like to compare the performance improvement of the Sejong corpus with the results of previous studies.

    We found that the proposed transformer-based reranking technique consistently improves the results of existing morphological analysis models. These results confirm that it opens up new possibilities by further improving the results of existing traditional machine learning models in the field of Korean morphological analysis. Finally, since the results of the major related studies proposed in the literature were mostly conducted on the Sejong corpus, we would like to compare the performance improvement of the Sejong corpus with the results of previous studies. Although it is difficult to make a direct comparison due to slight differences in implementation conditions and evaluation criteria, it can be seen that the proposed dictionary-based morphological analysis model is not up to the latest research results, but by incorporating a re-ranking model, it can secure performance that is comparable to the existing research.

    However, the entire morphological analysis model including the reranking model is not suitable for real-time processing, but it is expected that by reflecting the cases whose ranks are changed through the reranking model as feedback to the dictionary-based morphological analysis model, it is possible to obtain near-improved morphological analysis performance. The improved dictionary-based morphological analysis model can then be used as input to the reranking model, so it is expected that a gradually improving morphological analysis model can be obtained through this iterative feedback loop.


    \section{Conclusion}\label{sec:conclusion}

    This research signifies a progressive stride in the realm of Korean morphological analysis by seamlessly merging conventional dictionary-based techniques with advanced deep learning methodologies. Our findings indicate that while relying solely on dictionary-based morphological analysis does not surpass the efficacy of some existing models, the integration of a BERT-based re-ranking system notably enhances the accuracy, establishing a new standard in this domain.

    While the enhancement in performance comes at the cost of augmented computational demand, the introduced methodology offers an intriguing avenue for continuous enhancements. This innovative amalgamation of classical dictionary approaches and cutting-edge machine learning methodologies paves the way for groundbreaking advancements in the intricate and multifaceted domain of Korean linguistic processing.

    Future endeavors in this domain should emphasize refining this harmonious integration to achieve even higher precision in morphological analysis while optimizing computational efficiency. Moreover, our observations hint at the potential of employing a probabilistic model to discern areas where inaccuracies are likely to arise, enabling the retrieval of more accurate interpretations from a narrower candidate pool. The parallels between this initiative and the challenges of translation quality estimation suggest that insights from the latter could further inform and bolster the efficacy of our approach.


    %\nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;
    \bibliography{references}

\end{document}
